run_name: 'mk_local_dev'
seed: 12345
device: 'cuda'
lr_policy: 0.0001              # learning rate of the policy network
lr_flow: 0.01                  # learning rate of the flow network
lr_back: 0.0001                # learning rate of the backward pass
weight_decay: 1e-7
use_weight_decay: false
scheduler: True
lr_shrink_lambda: 0.9995
gradient_norm_clip: 1.0        # gradient norm clip for updating gfn model weights

# model parameters
zero_init: false

joint_layers: 4              # number of layers in forward/backward policy models
hidden_dim: 128              # dimensionality of hidden layer
s_emb_dim: 32                # dimensionality of state embedding
t_emb_dim: 32                # dimensionality of time embedding
harmonics_dim: 32            # dimensionality of harmonic embedding
condition_emb_dim: 32         # dimensionality of the conditioning embedding
dropout: 0.1                   # dropout probability
norm: layer                   # null, "layer", "batch"

batch_size: 25               # number of samples per training batch
grow_batch_size: true        # whether to dynamically grow the batch size to fill the RAM
max_batch_size: 50           # maximum batch size to allow
eval_batch_size: 500        # batch size for evaluation
epochs: 10000000             # number of training epochs: relatively large
eval_period: 250              # how often to do evaluation & reporting
figs_period: 500             # how often to log custom figrues
buffer_size: 100000          # size of replay buffer: relatively large

# GFlowNet settings
T: 10                        # diffusion trajectory length
t_scale: 1.0                 # scales learned policy variance
log_var_range: 4.0           # controls dynamic range of forward policy variance
pb_scale_range: 0.1          # range of learned backward policy variance
energy: "molecular_crystal"  # "molecular_crystal"
mode_fwd: "tb"               # "TB" is trajectory balance, "cond-tb-avg" is conditional VarGrad
mode_bwd: "tb"               # "TB" is trajectory balance, "cond-tb-avg" is conditional VarGrad
repeats: 10                  # for conditional VarGrad training
both_ways: false              # forward and backward training
bwd_batch_multiplier: 5      # ratio of backward to foward batch size (much cheaper to evaluate)
fwd_train_each: 1            # how often to do forward training compared to backward
bwd: false                   # backward training only
learn_pb: false               # whether to learn a backward policy
conditional_flow_model: true # PROBABLY WILL NOT RUN IF FALSE, conditional training (currently doing temperature)
learned_variance: true       # whether to learn policy variance
partial_energy: false
clipping: true  # clip the maximumstep size induced by the policy model
gfn_clip: 10.0  # clip the maximumstep size induced by the policy model

# local search settings
################################################################
buffer_path: 'D:/crystal_datasets/urea_gfn_dataset.pt'
molecules_path: 'D:/crystal_datasets/test_qm9_dataset.pt'

# replay buffer / dataset settings
################################################################
beta: 1.0             # reward prioritized replay sampling: high beta = steep prioritization
rank_weight: 1     # rank-based replay sampling: low rank_weight = steep prioritization
prioritized: "boltzmann"   # three kinds of replay training: random, reward prioritized, rank-based
sampling: "buffer"

################################################################

# exploration controls
exploratory: true
exploration_factor: 0.35
exploration_wd: true


# molecular crystal settings
################################################################
# scales the energy function
anneal_energy: false  # harden intermolecular repulsion over time
energy_annealing_threshold: 1.0e-2
convergence_history: 1000
energy_density_coeff: 10  # how much to weight the density penalty term in the energy function
temperature_conditioning: true
energy_min_temperature: 0.01
energy_max_temperature: 10
energy_static_temperature: 0.1
temperature_scaling_factor: 1  # controls relative frequency of higher vs lower temperatures (higher -> higher)


# deprecated or un-used

max_iter_ls: 100                # DEPRECATED # how many iterations to run local search
samples_per_opt: 10             # DEPRECATED #how many noisy samples to take near each optimized minimum
burn_in: 100                    # DEPRECATED # NOT USED IN MXTAL how many iterations to burn in before making local search
ls_cycle: 100                   # DEPRECATED # how frequently to make local search
ld_step: 0.1                    # DEPRECATED # langevin step size
ld_schedule: true               # DEPRECATED # whether to use scheduled langevin step size
target_acceptance_rate: 0.574   # DEPRECATED # target acceptance rate for local search
langevin: false
langevin_scaling_per_dimension: false
lgv_clip: 100.0
lgv_layers: 4
eval: false
pis_architectures: false # false goes to our custom architectures, which are better for training bigger models
subtb_lambda: 2              # subtb currently deprecated
local_search: false             # DEPRECATED we now combine preloaded and sampled elements into a single buffer
